# Retrieval-Augmented Generation
The Impact of Larger Context Windows: Large models with bigger
context windows can process a wider range of text, raising the question of
whether to provide the entire set of documents or just the relevant
information. While providing the complete set of documents allows the
model to draw insights from a broader context, it has drawbacks:
1. Increased latency due to processing larger amounts of text.
2. Potential accuracy decline if relevant information is scattered
throughout the document.
3. Inefficient resource usage, especially with large datasets.

When deciding between providing the entire set of documents or just the
relevant information, consider the application’s requirements and
limitations, such as acceptable latency, desired accuracy, and available
computational resources.

## What are Text Splitters and Why They are Useful
Text splitters are an important tool for efficiently splitting long documents
into smaller sections to optimize language model processing and enhance
the effectiveness of vector store searches. As we previously covered,
providing external information to LLMs (generally split into chunks) can
diminish the likelihood of hallucination, allowing users to cross-reference
the generated response with the source document.

A challenge in LLMs is the limitation of input prompt size, preventing them
from including all documents. However, this can be resolved using Text
Splitters to divide documents into smaller parts. Text Splitters help break
down large text documents into smaller, more digestible pieces that
language models can process more effectively. Here are the pros and cons
of this approach:

Pros:
1. Reduced Hallucination: Providing a source document to the
LLM guides its content generation process based on the provided
information, reducing the probability of fabricating false or
irrelevant information.
2. Increased Accuracy: When equipped with a trustworthy source
document, the LLM is more capable of producing precise
answers, a feature particularly valuable in scenarios where
accuracy is of utmost importance.
3. Verifiable Information: Users can cross-reference the content
generated by the LLM with the provided source document,
ensuring the reliability and correctness of the information.

Cons:
1. Limited Scope: Relying solely on a single document for
information can restrict the breadth of content the LLM
generates, as it only draws from the data within that document.
2. Dependence on Document Quality: The quality and
authenticity of the source document significantly influence the
accuracy of the LLM’s output. The LLM will likely produce
misleading or incorrect content if the document contains
erroneous or biased information.
3. Inability to Eliminate Hallucination: While using a document
as a reference can reduce instances of hallucination, it doesn’t
entirely prevent the LLM from generating false or irrelevant
information.

A Text Splitter can also enhance vector store search results, as smaller
segments might be more likely to match a query. Experimenting with
different chunk sizes and overlaps can be beneficial in tailoring results to
suit your specific needs.

### Customizing Text Splitter
It is necessary to divide significant texts into smaller, digestible portions
while managing them. This process can become complicated when retaining
the integrity of semantically connected text parts is critical. Note that the
definition of “semantically related” varies depending on the type of
material.
Text segmentation typically involves the following processes:
1. Breaking the text into smaller, semantically meaningful units,
often sentences.
2. Aggregating these smaller units into more significant segments
until they reach a certain size, defined by specific criteria.
3. Once the target size is achieved, the segment is isolated as a
distinct piece. The process is repeated with some segment
overlap to preserve contextual continuity.

In customizing text segmentation, two key factors must be considered:
* The technique for dividing the text.
* The criteria used to determine the size of each text segment.
Below, we discuss the techniques and the criteria they employ to determine
the size of the chunks.

Begin by cleaning your data and removing unnecessary elements like
HTML tags from web sources. Next, experiment with different chunk sizes.
The ideal size will vary based on the nature of your data and the model
you’re using. Evaluate the effectiveness of each size by running queries and
analyzing the results. Testing several sizes to identify the most suitable one
may be necessary. Although this process can be time-consuming, it is a
crucial step in achieving the best outcomes for your project.

### TokenTextSplitter
The TokenTextSplitter offers a key advantage over splitters like the
CharacterTextSplitter by ensuring that token boundaries are respected,
preventing the division of tokens midway. This feature is especially
beneficial in preserving the semantic integrity of the text, an important
consideration when working with language models and embeddings.

This splitter first converts the input text into BPE (Byte Pair Encoding)
tokens and then segments these tokens into smaller chunks. After
segmentation, the tokens within each chunk are reconstructed and put back
into text.

A potential downside of the TokenTextSplitter is the increased
computational effort required to convert text into BPE tokens and vice
versa. For quicker and more straightforward text segmentation, the
CharacterTextSplitter may be a preferable option, and it offers a more direct
and less computationally intensive approach to dividing text.

## Embeddings
Embeddings are dense vector representations that capture semantic
information, making them highly effective for various machine learning
tasks, including clustering and classification. They translate semantic
similarities perceived by humans into measurable closeness in vector space.
These embeddings can be generated for multiple data types, such as text,
images, and audio.

For textual data, models like the GPT series and LLaMA can create vector
embeddings for words, sentences, or paragraphs within their internal layers.
Convolutional neural networks (CNNs) like VGG and Inception can
produce embeddings for image data. In contrast, audio data can be
transformed into vector representations by applying image embedding
techniques to visual representations of audio frequencies, such as
spectrograms. Generally, deep neural networks can be trained to transform
data into vector form, resulting in high-dimensional embeddings.

***
A multi-modal vector store, can store various data types, including images, audio, videos, text,
and metadata.
